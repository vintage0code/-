# 王道数据结构理解和心得

[TIPS：发现网课过程中会有不理解的地方想要即刻消化，为了节省时间，有不理解的地方先在这里把问题记下来，等看完了这一章节再集中消化，因为这些知识前后有关联，汇合起来消化速度更快。争取一章节内容一早上消化完成。]()



## 第三章：数组

### 特殊矩阵的压缩存储

- 主要分为四种矩阵的压缩存储：对称矩阵、三角矩阵、稀疏矩阵、三对角矩阵。在物理存储的策略上，又分为按行存储和按列存储。做题来讲，`最大的建议就是熟悉几种矩阵的形式和掌握一般计算策略`。对于后者，`一般策略就是先计算前n-1行或列的元素总数，再加上当前行或列到指定位序的元素个数`，最后相加可以得到指定位序之前元素个数的总数，其实也恰恰是物理存储的索引位置（要看清楚题数组是否从0开始索引）。结合其矩阵形式和存储要求，按照一般计算策略能够现推出关系式。
- 稀疏矩阵考得较少，压缩存储的方法有两种：三元组法（行、列、值）和十字链表法。

## 第四章：串

### 1、串的定义

- 在串的比较操作中，需要将两个串按字符一一比较，`若字符第一次不同，则位序高的字符所在字符串大`，如字符A>B，则字符串M>N。若第一个字符串的`整体都是第二个字符串的真子串`，那么`长度更长的字符串大`。

### 2、串的存储结构

- 在串的顺序存储中，一般将`首位弃之不用`（保证索引和实际位序相同，更方便），然后加入一个变量length来记录串长。

### 3、串的模式匹配（KMP算法）

- `KMP算法的思想其实是从暴力解中优化而来的，主要优化的地方是主串指针无需每次一定重头回溯了`，这可以保证跳过大部分无意义的无结果的匹配比较。

- KMP算法的特性是与主串无关，`而与模式串有关。即模式串改变，回溯模式也要相应改变`。这里的回溯模式主要是被定义到一个`NEXT数组`中，其含义是模式串`若在k的位置上失配`，则`模式串指针应该回溯到定义好的NEXT[k]位置`，这个规律对于同一模式串是固定的。

- 关于`NEXT数组的求解`，这里只给出考试解方法，因为代码法考察不多。其求解思路大概是`在失配的位置将模式串逐一向后再配`一下看回溯到哪里是最好的。技巧上，由于必然性，NEXT[1]和NEXT[2]恒为0、1。

- KMP的进一步优化主要优化了NEXT数组。`考虑一个模式串中有相同的字符`，若失配在这个相同字符的位置出现，`由于有的NEXT直接回溯到另一个相同字符`，则在该种情况下，因为此时主串指针的元素`必然与模式串失配位置的元素（即相同字符元素）不相等`，NEXT回溯后必然会失配，为了节省这次失配无意义的计算，`不妨改NEXT[j]直接跳到失配后的回溯位置`，修改后的NEXT数组称为NEXTVAL数组。

- 考试中的NEXTVAL数组求解主要用列表法，如下表，其本质和上条思想一致。`先查NEXT表，回溯发现元素相同就改表到NEXTVAL。`

  |  操作对象  |  /   |  /   |  /   |  /   |  /   |  /   |
  | :--------: | :--: | :--: | :--: | :--: | :--: | :--: |
  |   序号j    |  1   |  2   |  3   |  4   |  5   |  6   |
  |   模式串   |  a   |  b   |  a   |  b   |  a   |  a   |
  |  NEXT[j]   |  0   |  1   |  1   |  2   |  3   |  4   |
  | NEXTVAL[j] |  0   |  1   |  0   |  1   |  0   |  4   |



## 第五章：树与二叉树

### 1、树的定义

- 当一个`树没有任何结点`时，我们称该树为`空树`。
- 树中结点的最大度数称为数的度。
- 术语双亲就是我熟悉的父结点的意义；`堂兄弟可以理解为A与B结点在同一层但非同一双亲所生`。
- `用度来对树的结点分类`：度数>0为分支结点，度数等于0为叶子结点（终端结点）。
- 深度或`高度是从某一结点往下数的`，但是若谈论某树的高度，则是规定树中结点最大层数，因此一般而言，`树的高度是根结点的高度（包括根结点这层）。`
- `路径是`A到B结点的遍历`序列`，路径长度是该路径上边的个数。`树的路径长度是`根结点到每个结点`长度的总和`。

### 2、树的性质

- 如下图，其中`度为m的树至少有m+1的结点`，对应的结点至少有几个的情况是，`该树只有一个度为m的结点`，`其他结点都是度为0的叶子结点`，很容易想到，这样的树只有两层，根结点一层，根结点的度就是m，根结点的孩子就是叶子结点。

  <img src=".\图片\屏幕截图 2024-08-15 090211.png">

### 3、二叉树

- 完全二叉树中，若某`结点编号为i小于等于总结点数n/2的向下取整`，则`该结点为分支结点`，否则为叶子结点。怎么理解：由于总结点数n/2的向下取整`是编号为n即最末的结点的父结点`，因此可以想到该父结点位于倒数第二层，因此i若大于该父结点的位序，则很有可能结点i位于倒数第一层，肯定是叶子结点了。当然，也可能还是在倒数第二层，因此还有一种理解就是，由于有序完全二叉树是按层次从左往右排序的，因此`可以把该父结点看作截止点，截止点之后的都是叶子结点`。

- 若`完全二叉树的结点个数为奇数`，则可以想到，`去掉根节点后所有结点都是满的`，因此分支结点左右子树完整。若结点个数为偶数，相应的，对于末尾的分支结点只有左子树。

- 完全二叉树的一个重要性质，即可以`通过判断完全二叉树的结点总数n的奇偶性来推出具体的每种度的结点的个数是多少`。推断如下：完全二叉树度为1的结点要么没有要么就只有一个，又通过添加一个度为2的结点数变量可以得到度为1和度为2的结点数之间的关系式，最后通过从结点总数n的奇偶性出发，再根据关系式的约束，倒推具体的每种度的结点的个数是多少，如下列公式。
  $$
  relations:\begin{cases} 
    n_1=0\or1 && \\
    n_0=n_2+1 \to n_0+n_2=2n_2+1 \\
    n=n_0+n_1+n_2
  \end{cases}
  feature:\begin{cases} 
    n_1=1,n_0=k,n_2=k-1 &\text{if }n=2k  &&& \\
    n_1=0,n_0=k,n_2=k-1 &\text{if }n=2k-1 \\
  \end{cases}
  $$
  

### 4、二叉树的遍历

- 二叉树遍历是单向不可逆的，`即给定一条遍历序列（包括层次遍历序列），不可能逆推出唯一的二叉树结构`，只有`引入中序遍历序列才能`唯一确定二叉树的结构。有了两条遍历序列，可以`利用分层分堆的方法还原`式的自上而下画出二叉树。
- `考研中，二叉树的遍历最好不要用程序思维，可以用分层确定的手写方法`或者画线法，这样对人类来说速度更快。

### 5、线索二叉树

- `线索二叉树问题的来源`是，在某棵静态树下得到某种遍历序列后，也许`某个应用需要频繁访问这种遍历序列`，而`传统访问必须要从根结点开始重新遍历`，当我只需要从遍历序列的指点出发遍历，该怎么办。
- `线索二叉树的前驱后继与一般二叉树的概念不同`，前者是基于某种遍历序列的，后者是基于二叉树的。
- 线索二叉树的`线索指针就是利用了二叉树中的空指针域来存储`，并规定`左孩子指针存前驱，右孩子指针存后继`，将`所有空域指针存满`即可，没有前驱或后继就置null。为了分辨一个结点的指针域到底是不是存的线索，因此`还在每个结点设立左右标志位`，是线索就标志为1。
-  `将一棵二叉树线索化`，其实现方法`在进行某种遍历的同时线索化`，因此从代码的角度上，遍历的代码逻辑基本不变，主要是访问函数当中实现线索化的逻辑，`具体是启用了全局变量指针前驱指针`，用于记录前驱和线索化时赋值给线索。最后`要注意序列的尾部没有后继`了，要`特别的将其后继指针设为null`。
- 第一点提到了，线索二叉树最常用的应用就是`从指定点出发向后遍历`，因此需要一种遍历序列后继的算法：①当指定点或当前遍历点的孩子`是线索结点`时，`直接访问其后继`就是；②当指定点或当前遍历点的孩子`不是线索结点`时，其后继不一定是相应的孩子。那么只能`按照朴素二叉树遍历`的思想，并且`分析遍历下来谁会是指定点或当前遍历点的后继`——举个例子，对于线索二叉树中序遍历的②号情况，指定点或当前遍历点的后继是其右子树最左下结点。总体来看，`线索二叉树的遍历是这两种情况的交替处理，直到遍历完成。`
- 线索二叉树的也可以用于`倒序遍历（向前遍历）`，基本思想与向后遍历方法差不多，但是`区别在于对于②号情况`指定点或当前遍历点的孩子不是线索结点时，`前驱的信息并没有被记录从而找不到`前驱，因此有`解决方法是`每个结点再`添加一个指向双亲的指针`，这时可以通过前一点的分析方法`对当前结点是双亲结点的左孩子还是右孩子进行分情况讨论`。

### 6、树的存储结构

- 有三种：双亲表示法（顺序存储）、孩子表示法（顺序+链式存储）、`孩子兄弟表示法（链式存储）`
- 孩子兄弟表示法有两个指针域，其中一个指针指向第一左孩子，第二个指针指向右兄弟。`注意，每个结点都有这两个指针域，因此每个结点都要尽可能找出孩子和兄弟存储起来。`
- `孩子兄弟表示法`从存储结构可以看出，`本质上就是二叉树`，因此，`对于任意一棵树用孩子兄弟表示法表示出来看起来都是二叉树`，如二叉树-->孩子兄弟二叉树，树或森林-->孩子兄弟二叉树，且`这是可逆的`，即孩子兄弟二叉树-->二叉树，孩子兄弟二叉树-->树或森林。
- 树与森林与二叉树的遍历等价表如下：
  <img src=".\图片\屏幕截图 2024-09-07 084113.png" style="zoom: 33%;" />
  其中主要突出xxx与相应孩子兄弟表示的二叉树的等价，比如森林的先序遍历序列，是先访问森林中最左边的树，再访问余下的树，那么森林转换成二叉树时，先序遍历的访问顺序也是先访问最左边的树化为的二叉树，而树的先序遍历与二叉树又相同，这为遍历等价提供了解释。根据此种解释，还可以推断出表格没有的结论：森林的后序遍历结果与二叉树中序遍历相同。
- #### 树、森林与二叉树易错、易混淆点：
  
  - 树、森林转换到二叉树后，他们之间有一些数量关系：
    ①树的叶子结点数量是转化而来的二叉树中左指针为空的结点数，因为森林或树中的叶子结点对应到二叉树中一定是没有左孩子的，右孩子有没有没有影响。
    ②转化而来的二叉树中右指针域为空的结点数是n+1，n是原来森林（如果是树的话，可以把对应的二叉树理解为森林的二叉树）中非叶子结点的个数。因为我们研究森林中的非叶子结点，该结点一定产生一个二叉树的空右指针域——非叶子结点最右边的孩子是没有兄弟的，因此右指针为空。再加上1，因为森林的最后一棵树也没有别的树做他兄弟了，所以最后一颗树的根结点算一个空右指针域。

### 7、哈夫曼树

- 哈夫曼树是`带权路径长度最小`的二叉树，因此`哈夫曼树是一个最优树`的概念，不是一般树。
- 给定任意权值的结点，可以`构造出哈夫曼树`；假设`给定结点数为n`，因为第一次合成操作需要用掉两个结点，剩下的结点依此和已经合成的结点结合，故总体来讲`合成操作的次数为n-1次`。
- `哈夫曼树的结点总数为2 n-1`，因为`每次合成操作会生成一个合成结点`，因为n-1次操作，所以就有n-1个合成结点，最后`加上给定的结点（叶子结点）`，就是n + n - 1 = 2 n - 1。
- `哈夫曼树不存在度为1的结点`，因为常用的构造算法是用的两两合成的思想，因此`每棵子二叉树都是满的`。
- `哈夫曼编码给出的编码长度可能是不等的`，另外`哈夫曼编码也是没有歧义性的`。编码长度不等是因为大多数情况下，需要编码表示的信息量远远小于该编码的极限，因此不等编码长度可以减少码长，一方面有利于存储，另一方面有利于阅读。没有歧义性指每个编码的前缀都是不同的，若相同，则存在包含关系，那么在解读时就造成歧义，而`哈夫曼编码是前缀唯一的，故又称前缀编码`。
- #### 哈夫曼树易错、易混淆点：
  
  - 哈夫曼树的意义依赖于带权路径长度WPL，而WPL的数值不是构造哈夫曼树时顶端的合成值——事实上那个合成值不是我们关注的权值，权值在叶子结点上，另外一般而言，权值是关于结点的而不是关于边的——WPL的数值是叶子结点的权×路径长度。
  - 哈夫曼树在构造时，不要将合成结点和剩余结点比大小，只需要将合成值放回剩余结点中，然后从中取出最小的结点。
  - 哈夫曼编码时，根结点是没有编码的含义的，这时因为编码取的是边，习惯设左分支边为0，而右分支边为1。
  - `哈夫曼编码的加权平均长度是 WPL / 结点权值总和。`
  - 定长编码的二叉树，其编码的码符都位于叶节点且在同一层，这是因为定长编码的长度都相等的原因导致的特性。



## 第六章：图

### 1、图的基本概念

- `图不可以是空图`，即顶点必须有至少一个，然而图的边可以是空集；圆括号表示无向图，尖括号表示有向图。
- `数据结构中的图有两个定义概念`需要注意：不存在重复边；不存在顶点到自身的边。
- `无向图的度的总数是边的总数乘以2`，因为每一条边都贡献了两边的一个的度。同样的，有向图的度的总数和无向图保持一致，且由于有向的原因，入度数=出度数，入度数+出度数=总度数。
- `简单路径：说人话就是直直的走路径`，每个地点只经过一次，与之相对应的就是折返的走路径，有的地点要打转走几遍。同样的，简单回路类似于简单路径的概念。
- `顶点到顶点的距离`：距离描述的是两点之间的`最短路径`。
- 有向图的强连通指的是两点之间`互相可通`。自然的，若对于任意两个顶点都是强连通的，称该图为强连通图，否则为非连通图。
- 无向连通图最少有n-1条边，因为只从一个结点出发就能连通所有结点，如右图：<img src=".\图片\屏幕截图 2024-09-01 103002.png" style="zoom: 25%;" />
- `无向非连通图最多有C(2, n-1)条边`，其中C表示组合操作符号，n-1是下标。若是连通图，则最多有C(2, n)条边，然而为了改造使其称为非连通图，同时尽可能的保留多的边，则需要去掉一个顶点的组合，即最多有C(2, n-1)条边。如右图：<img src=".\图片\屏幕截图 2024-09-01 104107.png" style="zoom:25%;" />
- `有向连通图最少有n条边，但需要注意的是，这只是必要条件`，即一个有向图最少有n条边时不一定是连通图，比如n-1个顶点都没有入度的情况。不过，如果`进一步限定`，最少有n条边且构成回路的，则必是有向连通图，如右图：<img src=".\图片\屏幕截图 2024-09-01 105411.png" style="zoom:25%;" />
- `子图是`建立在`边和顶点一起的子集`的概念。`生成子图`继承母图的全部顶点，但边不一定全部继承。
- `图的连通分量`：说人话就是`把一个图分成若干不相关的子图`，其中`每个子图都是连通`的，且`尽可能有多的边和顶点`，称这样的子图为连通分量。这样的目的在于，将一个图解耦，分解最大的互不相关的子问题，从而各个击破。
- `无向连通图的生成树`：`保留所有顶点，然后保留最少的边`以刚好满足连通的特质。无向连通图的生成树`有一些变化上的性质`：若`砍去其一条边`，则变成了非连通图，若`加上一条边`则会形成一条回路。由于生成树连通且没有环路，因此本质上来说是一棵真正的树。
- `带权路径长度`：一条路径上的所有边的权值之和。
- `完全图`：说人话就是`顶点全部连满了，有点像全连接`的感觉。值得注意的是，`有向完全图共有2*C(2, n)条边`，因为每两个顶点之间有出入的两次连接，然后共C(2, n)种组合。
- `无向图可以和树互相转换`：不存在回路，且连通的无向图可以变成树。`也有一些变化上的性质`：加上一条边则会形成一条回路。
- #### 图的易错、易混淆点：

  - `无向图中没有强连通概念`，代替的就是连通概念，强连通概念主要用于有向图。
  - 强连通分量就是连通分量的概念，需要强调的是`连通分量都是极大的`，即尽可能多、尽可能的合并，因此并不是找有几个回路就行。
  - 加强对无向图中的树和森林的理解。
  

### 2、图的存储及基本操作

- 邻接矩阵存储法，分为带权矩阵和非带权矩阵。`有时矩阵会定义将自己到自己的边设为0。`

- 非带权矩阵有一个很好的性质，`矩阵的n次方构成图的路径长度为n的路径数目分布矩阵`，比如矩阵的2次方取[1] [4]的值，是路径长度为2的从1号顶点到4号顶点的路径数目。该性质的原因是，因为矩阵乘法规则刚好使拥有连续路径而形成一条长路径的可以被计数，如下图：<img src=".\图片\屏幕截图 2024-09-03 091508.png" style="zoom: 50%;" />

- 怎么`理解性记忆邻接表？`邻接表将`顶点和边分开表示`，然后`耦合存储`。其中`顶点采用顺序表，边采用链表`。边在结构体的定义中，实际存储的是顶点顺序表的顶点索引，理解为从顶点A到顶点B为一条边。邻接表如图：<img src=".\图片\屏幕截图 2024-09-03 112945.png" style="zoom:33%;" />

- `邻接表和邻接矩阵对比`如图：
  
- <img src=".\图片\屏幕截图 2024-09-03 094601.png">
  
- 为了解决邻接表计算有向图入度和入边的不方便，经过改进有了十字链表。十字链表的思路是增加了记录入边的路径，即从任意一点，既可以选择走以该点为起点的路径，又可以选择走该点为终点的路径。如图：其中弧头表示箭头指向的顶点。弧结点数据结构的下部数据域记录了上述的两种路径，用来寻路。上部数据域直接表达边，即左顶点到右顶点的路径。`（不考）`
  <img src=".\图片\屏幕截图 2024-09-03 105546.png">
  
- 为了解决邻接表存储无向图的冗余存储（A-B存1次，B-A又存1次——在无向图中这种方向感无用，所以可以将两次存储合并为1次——而且在图的一次操作中对应需要修改多处）缺陷，对照十字链表，经过改进有了邻接多重表。邻接多重表主要是边结点的数据结构做了改进，其中最重要增加了依附于A的下一条边、依附于B的下一条边的指针，正是这种方便修改的指向特性使实际上为一条边的两条边合一存储。

- 最后，除了邻接矩阵外，其他的存储结构在表示同一张图时不唯一。主要原因在于顶点顺序不同，例如，如果我们有一个无向图，顶点集为 {A, B, C}，那么顶点可以按照 [A, B, C] 或者 [C, A, B] 等不同的顺序存储。

- 在图的邻接矩阵存储结构下，删除某个顶点就是把对应顶点的行列置为零，并在顶点域设置空标志。

- #### 图的存储易错、易混淆点：
  
  - `有向图的邻接矩阵计算入度是查矩阵列，而不是查矩阵行`。
  - 有向图的邻接表中，`顶点v在边表中出现的次数是顶点v的入度`，`因为边表是不包括顶点表的`，任何在边表中存放的顶点都是以顶点表中顶点为起点的边所对应的另一个顶点v。从而v在边表中出现的次数也就是它的入度。

### 3、图的遍历

- 广度优先算法基本思想就是依此访问相邻的所有顶点。
- 值得一提的是，最容易想到的是连通图的遍历方法，但`也有可能给定图是非连通的`，那么改进相应的算法，通过在`一次遍历结束后检查标记数组`，若全都访问过了则证明该图是连通图，算法可以真正结束；否则证明该图是`非连通图，于是跳转到第一个未访问的结点再遍历`。故该算法产生了`额外的一个性质`：对于`无向图，执行了多少次遍历算法`，就说明该图`有多少的连通子图`（连通分量数）。
- 图的深度优先算法其实和树的深度优先算法（先根遍历）基本一致，`都是递归调用来完成遍历`。
- 在图的遍历算法中，`时间复杂度的计算可以被固化`：时间复杂度=访问各顶点所需时间+探索各条边所需时间。不同存储结构有不同复杂度。
- 图的遍历易错、易混淆点：
  - 广度优先和深度优先的操作分别依据了队列和递归调用的栈，因此分析空间复杂度时，从队列和递归调用入手。其中广度优先的空间复杂度为o(n)，因为最坏的情况如下图，只有一层，但是所有结点都在这一层了，因此需要全部入队。
    <img src=".\图片\屏幕截图 2024-09-09 090035.png" style="zoom:33%;" />
  - 无论是广度优先还是深度优先，无论其存储结构如何，算法都需要先访问顶点，再去遍历得到该顶点各边的信息，这种其中主要是深度优先遍历需要理解：深度优先是递归访问的，这个递归访问的次数刚好就是将顶点访问完的次数，其中每个递归内要遍历边。

### 4、最小生成树

- `最小生成树主要应用于带权连通无向图`，在生成树的概念下增加了带权最小的要求。

- `如果一个连通图本身实质上是一棵树`，那么其最小生成树就是本身，`因为其图权没有选择的余地`。

- `最小生成树分为Prim算法和Kruskal算法`。两个算法的区别在于，前者依赖一个初始顶点，并且以顶点集作为决策依据；后者从全局最小的权值边出发，以边集作为决策依据。`一个是通过增加顶点、另一个是通过增加边来探究最小生成树，另外算法效率也不同`。

- `Prim算法的做法`是：从某一个顶点开始构建生成树，每次将代价最小（如权值最小）的`新顶点纳入生成树`，直到所有顶点都纳入为止。如下图中的左图。`Kruskal算法的做法是`：`每次选择一条权值最小的边`，使这条边的两头连通（原本已经连通的就不选），直到所有结点都连通。如下图中的右图。

  <img src=".\图片\屏幕截图 2024-09-05 131012.png" style="zoom: 33%;" /><img src=".\图片\屏幕截图 2024-09-05 132330.png" style="zoom:33%;" />

- `Prim算法的实现思想`：维护两个数据结构，第一是顶点纳入的`标志表`，第二是`已纳入生成树顶点集`与`暂未纳入生成树顶点之间`的`最小代价表`。`每次迭代纳入一个顶点`，再马上根据新的生成树来更新最小代价表中变动的代价，保证代价表每个顶点都是代价最小的...周而复始。

- #### 最小生成树易错、易混淆点：

  - Kruskal算法并不像Prim算法是纳入式迭代的，Kruskal算法怎么样都是选权值最小边，只要该边两头还没连通，就继续迭代。可以想象成，每次选一条边后，双端顶点变灰，已经变灰的就不选。

### 5、关键路径

（关键路径根据考纲，还是有可能会考到，因此需要找补）

### 6、最短路径问题

- 最短路径问题的分类如图：
  <img src=".\图片\屏幕截图 2024-09-07 104628.png" style="zoom: 33%;" />
  其中单源最短路径指的是指定的一个起始顶点到其他所有顶点之间的最短路径。
  
- BFS求解单源最短路径的灵魂在于维护两个数据结构：一个是到每个顶点的距离表，另一个是每个顶点在路径中的直接前驱表。

- Dijkstra算法思想和最小生成树差不多，计算每次迭代确定下来的路径带来的改善，使下次迭代可以利用改善的结果直接确定最短路径，再计算新的改善...维护三个数据结构，一是起始顶点到各顶点的最短路径是否找到的标记表；二是起始顶点到各顶点的最短路径长度表（路径长度指的是该路径下边权值总和）；三是每个顶点在路径中的直接前驱表。算法的大循环中的每次迭代都能确定到一个顶点的最短路径，因此该迭代不需要再改动已经确定最短路径的顶点。然后马上计算，对于该顶点暂未确定最短距离的直接邻点，从起始顶点出发，在经过该顶点的情况下，其确定的路径能不能改善到直接邻点的最短路径（最短路径长度表就是用来维护这一过程的），并实施相应的改动。
  [（如果以后复习时看文字不能有效想起来算法的特点，那么证明文字还是不清楚，到时候重新插图）]()
  
- Floyd算法思想是一种动态规划的思想，这里的子问题定义为求各个顶点之间的最短路径，子问题递进的条件是逐个增加中转顶点，直到所有顶点添加完毕。总的来说，算法维护两个数据结构，一是记录各顶点间的最短路径长度（n×n的矩阵），二是记录各顶点之间路径的中转节点编号（n×n的矩阵）。算法的大循环是逐渐增加中转顶点，之后计算在增加中转顶点后，各个顶点之间的最短路径是否有改善的。具体操作是逐个遍历矩阵所有元素，取出相应的上次记录的最短路径，比较当前加入中转顶点的路径是否更短，并进行相应的更新。大循环执行n次，而内部的矩阵遍历执行n的平方次，因此总的时间复杂度是n的三次方。
  值得一提的是，记录各顶点之间路径的中转节点编号矩阵在每个元素位置上只记录了一个中转节点，因为图完全有可能有多个中转节点，所以看起来好像该矩阵丢失了其他中转节点的信息，实际上其他中转节点的信息被隐含在了唯一记录的那个中转节点中，于是可以通过回溯来寻找其他中转节点的信息。有点像链表，单个结点我只需要记住离我最近的结点位置就行了，连起来照样能寻头找尾。
  
- #### 最短路径易错、易混淆点：
  
  - 单源最短路径的算法也可以求任意两顶点间的最短路径，因为如果逐一把单源出发的顶点运行一遍单源算法，将算法结果综合起来就近似的求出了任意两点之间的最短路径，只不过这样没有Floyd算法效率高而已。
  - 当题目让你依据图来找出最短路径，可以直接每个选项带入看哪个最短选哪个，一方面快，另一方面避免笔算的差错，再不行才用算法画表的方法来找路径，复杂图切勿用眼睛看。
  - 有向无环图的拓扑序列唯一并不能唯一确定该图。因为宏观来讲，拓扑序列唯一不仅因为分支为0，也因为同步锁定。如下图所示：
    <img src=".\图片\屏幕截图 2024-09-11 205549.png" style="zoom:50%;" />
    左图是因为同步锁定的原因才序列唯一，而右图是分支为0的原因才序列唯一。
  - 题目中有要计算Dijkstra算法依此得到目标顶点的，要用列表法按算法一个个确定最短路径的顶点，就是依此得到的目标顶点序列。其中需要注意的是，每当我们执行“经过当前顶点的情况下，其确定的路径能不能改善到直接邻点的最短路径”的步骤时，要记得经过当前顶点的距离是查表确定的，不然有可能会眼睛看成直线距离，而直线距离不一定是要求的当前最短距离。

### 7、拓扑排序

- 拓扑排序的来源是，想要对用图表示的流水线操作进行排序，这种序列是合乎先后步骤的，不可逆转的，因此该图必须是有向无环图。

- 根据流水线操作图的特点，序列一定是由入度为0的顶点开始、出度为0的顶点结束的，一个基本的排序算法是，维护一个入度表，利用栈存储入度为0的顶点，每次弹出时就相当于排好了一个元素，并且删除该元素顶点的出边。若再遇到入度变成0的顶点就再压入栈，循环往复。

- 逆拓扑排序的算法主要是深度优先算法，利用了深度优先一条路走到黑的特性，走到尽头就说明该顶点出度为0，因此可以将前面用栈存的顶点顺序逆序弹出了。

- #### 拓扑排序易错、易混淆点：

  - 用邻接表的存储结构实现的拓扑排序算法的时间复杂度为o(n+e)。其中n是指每个顶点都会处理一次，而每个顶点附庸的边都会被处理一次，故加起来就是o(n+e)。值得一提的是，每个顶点下确实需要遍历一遍它的边，这并不意味着o(n * e)的复杂度，o(n * e)的意思是每个顶点下都要完整访问图中的所有边，但在邻接表的情况下，不需要在每个节点下访问图中的所有边。



## 第七章：查找

[从本章开始的查找和排序算法，864似乎考的频次高，要注意重点吸收和做题。]()

### 1、查找的基本概念

- 查找长度：在查找运算中，需要对比关键词的次数称为查找长度。
- 平均查找长度(ASL)：所有查找过程中进行关键词比较次数的平均值，(很重要，常考)，如图：<img src=".\图片\屏幕截图 2024-09-13 082931.png" style="zoom: 33%;" />
  其中的概率若没有元素重复或特殊的说明，一般是均匀分布概率，即查找每个元素的概率相同。举个例子，对于二叉排序树，每层结点的查找长度相同，都等于层高。因为二叉排序树规定左数<中数<右数，因此从一个根结点出发，很清楚的知道该往左边走还是右边走，无需先向左边走，比了后再向右边比。根据这一特性，二分查找算法的ASL类似。
- 平均查找长度(ASL)反映了查找算法的时间复杂度。

### 2、顺序查找

- 顺序查找指数据结构为线性表的查找，如顺序表、链表。顺序查找的基本思想也非常朴素，就是一个个找。
- 顺序查找算法实现中有一个比较妙的实现方式，就是将表头挖出来，放置待查找的元素，然后从尾到头查找，这样即有利于迭代条件更清晰，又不需要对查找失败额外写逻辑进行判断，因为找到表头就相当于查找失败，而表头的位置索引刚好是0，和查找失败含义相同。
- 可以对顺序表查找优化以提高一点点效率，如将元素有序存放，如下图，就不需要每次从头到尾查了。
  <img src=".\图片\屏幕截图 2024-09-13 092223.png" style="zoom:25%;" />
- 查找失败就是比对到紫色矩形的时候，换个角度来看就是空指针域，而空指针域对于任何一棵二叉树来讲数量为n+1，因此就会有n+1中查找失败的情况。值得一提的时，对于任何使用构造二叉树来查找的算法，都是n+1的失败情况，比如二分查找。

### 3、二分查找(折半查找)

- 二分查找仅适用于有序的顺序表存储的数据，因为只有顺序表具有随机访问的特性，只有利用这个特性才能二分。

- 二分查找算法主要利用指针进行二分操作，分为low和high指针（low一般从0开始），分别指向表头和表尾。二分操作的迭代条件是mid=low+high/2的向下取整，当查找数小于mid所指元素时，将high指针设为mid-1，即mid右边一位元素，查找数小于mid同理，之后循环往复。迭代结束条件分为查找成功结束和查找失败结束，其中查找失败结束条件是low指针大于high指针（迭代到指针错位的时候）。

- 二分查找算法本质上就是在构造判定树以进行查找，研究判定树不难发现一个性质，若限定mid向下取整的构造方法，每次二分后，对于奇数个元素的顺序表，左半部分比右半部分少一个元素；对于偶数个元素的顺序表，两半元素树相等。因此总的来说，每次二分后左半部分比右半部分少一个元素或零个元素。因此换到二叉树的视角来看，每个结点及其儿子结点都满足这个性质，因此刚好满足平衡二叉树的定义。有了这个性质，对于任意一个顺序表可以构造一个唯一的判定树。

- 又由于二分的原因，判定树前面层一定是满的，只有最后一层可能不满（如奇数个元素的顺序表不满），因此某种角度等价于完全二叉树。

- 二分查找的mid也可以向上取整，本质上都是极其类似的，只是实现上不同而已。

- 二分查找的查找效率ASL与构造树的层高有关，具体的说法在1、中有例子。

- #### 二分查找易错、易混淆点：

  - 不要把关键字与比较关键字以及元素序号mid搞混淆了，他们的关系是arr[mid]=比较关键字，然后比较关键字和关键字执行比较。
  - 二分查找与二叉排序树之前分析过ASL基本一致，这里指的是思路一致，但两者的效率仍有不同的时候，比如对于同一序列，普通二叉排序树可以很高，不平衡；但是由于二分查找的判定树必定是平衡二叉树，所以高度远小于此时的排序二叉树，并造成效率差异。
  - 若给定序列，那么二分查找的判定树唯一，因为一方面有判定树的约束，另一方面各个元素的大小确定了，因此作为结点在判定树中的位置也是唯一确定的。反之若未给定序列，只给定了元素数量，虽然仍可以构造判定树，但是由于各个元素的不确定，因此元素位置不确定，那么这时判定树就不唯一。最明显的就是叶子结点可以按照规则有多种位置。
  - 有一类题目让选择正确的判定树，其考点是利用判定树的性质，然而因为那性质有向上取整和向下取整的区别，因此两种情况都是成立的。值得一提的是，同一棵构造树不能既有向上取整的性质又有向下取整的性质，因此中心对称的构造树一定是错误的。

### 4、分块查找

- 分块查找的思想是将顺序表分成k块，块中元素数量不限、顺序不限，每块中取最大元素作为键存放到索引表中，同时索引表还存每块的区间，只要确定了查找元素落在对应区间的块，就能够下沉到块中，进行块查找。因此分块查找具有块内无序，块间有序的特点。整体思想其实和图书馆书册分类神似，是一种多级索引思想。
- 如果使用二分查找的方法来查找索引表，那么查找成功需要满足两点，首先是索引表的查找成功：条件是low指针大于high指针时，那么就进入low指针相应的区块进行下级查找，因为考虑到low指针大于high指针的前一步必然是low指针等于high指针等于mid指针，这时，按照指针的移动规则，无论移动low指针还是high指针，结果都是low指针大于high指针，而且查找数都是落在low指针区间内。其次就是区块内的查找成功，这部分查找只能通过顺序遍历。
- 查找失败也有两种原因，首先是当low指针大于high指针时，但low指针却指向不存在的区块（这种要么是指到了索引表最左边要么就是最右边），那么就是查找失败，因为相当于查找数没有任何一个区块符合它。另一种是下级查找时，查到尾部还没有那就是查找失败。
- 对于非均匀分块的分块查找算法我们难以用通项公式来描述其时间复杂度，但是假设均匀分块，那么时间复杂度就可以通项计算，如下图：
  <img src=".\图片\屏幕截图 2024-09-13 142902.png" style="zoom:33%;" />
  其中概率用的是均匀概率。对于上图的ASL，可以转化为极值问题求出优解。若采用二分法查索引表，则时间复杂度如图：
  <img src=".\图片\屏幕截图 2024-09-13 143454.png" style="zoom:33%;" />
  其中L1来源于其二分查找构造树的查找效率。

### 5、二叉排序树

- 二叉排序树的查找算法分为非递归方法和递归方法，非递归方法因为是一堆if else，故空间复杂度为o(1)；递归方法的复杂度因为涉及调用栈，故最坏空间复杂度为o(n)。
- 二叉排序树的插入操作中，新插入的结点一定是叶子结点。因为插入的元素一定需要向下探索，直到找到一个合适的空位，即叶子位置。
- 二叉排序树的构造算法，可以视为插入操作的不断进行。
  - 二叉排序树的删除操作，分为几种情况：
    （1）若需删除结点为叶节点，则可以直接删除，不影响排序树的特点；
    （2）若需删除结点只有一棵左子树或右子树，则可以直接删除需删除结点，再将其左子树或右子树顶替，因为其左或右子树必然小于或大于需删除结点的双亲结点，因此该操作一定符合排序树的特点；
    （3）最后一种情况是，若需删除结点有左子树且有右子树，根据排序树的特点即中序遍历，可以得出需删除结点的比他大的下一个元素在其右子树的最左下处，用替补式删除法讲需删除结点替换为最左下处结点。而最左下处结点也需要删除，因为它被替补过去了，由于最左下处结点已经最左下了，故该结点一定没有左子树，因此可以使用前两种情况的删除方法来处理。参考图例如下：
    <img src=".\图片\屏幕截图 2024-09-17 230305.png">
    
    最后，除了像上述的后继替补的删除法，相应的，前驱替补的删除法也同样有效，那么这时就需要找到需删除结点左子树中最右下结点，其他细节基本与后继替补的方法类似。

### 6、平衡二叉树

- 一个使用二叉树平衡算法的充分理由是，由于二叉排序树的查找效率与树高有关，而二叉排序树的构建算法很容易构建出大幅失衡的排序树，或者在排序树的插入操作中，若插入n次，最坏的情况下会使得树相应增加n层，也会使排序树大幅失衡，造成查找效率降低。

- 二叉树平衡算法引入一个平衡因子的数据项，用来评估二叉树每个结点的平衡性，若平衡因子绝对值大于1，则表示失衡。平衡因子的一般定义是左子树高-右子树高，题目中默认是这个计算方向。

- 本节涉及的经典二叉树平衡算法基于一个特性：在插入操作当中，只要将最小不平衡子树调整平衡，则其他祖先结点都会恢复平衡。因为若插入导致整体树高+1，那么连锁反应会使得上层结点有失衡的，可以说一次插入是一次失衡的唯一原因，因此调整平衡就可以恢复。

- 二叉树平衡算法分为四种情况来处理，如图：
  <img src=".\图片\屏幕截图 2024-09-19 103049.png" style="zoom: 33%;" />
  根据上图可知，在处理平衡调整时，一关注失衡节点，二关注插入结点与失衡节点的关系，以便判断调整类型。其中C要强调它是子树的概念，我们不关心C这棵子树多高多大，我们只关心C是在B的左边还是右边。值得一提的是，A、B都不是子树的概念，本质是单独的结点。

- 那么根据四种失衡的类型，便可以制定相应的策略来调整，如图：
  <img src=".\图片\屏幕截图 2024-09-17 110928.png" style="zoom:33%;" />
  总的策略方针就是，调整后的有序树仍然保持有序的特性，以及我们所努力维持的平衡。图中只要考虑有序性，上层就很容易推下层。

- 有一个特殊的地方需要注意，如下图拿RL调整的例子来说，根据上图RL型调整的模型，我们可以想到从结点7和结点11的角度来说，结点8无所谓归置到结点7的右下或者结点11的左下，然而，若从结点9的角度来说，结点8只能归置结点7的右下，这样整棵树才有序。
  因此，我们应该要牢记关键的两点：

  - 对于排序二叉树，【左<中<右】的规则是贯彻到各个层次上的，比如说作为最大的层次，根结点所代表的二叉树，其左边的任何结点一定都小于根结点，自然有右边的任何结点一定都大于根结点，其他层次同理。
  - 平衡调整操作后的结果树是唯一的，因为需要保证结果树的所有结点都有序，而这样的严格有序是不可能造成存在一个结点有别的排序可能性的。

  <img src=".\图片\屏幕截图 2024-09-19 104755.png" style="zoom: 33%;" />
  
- #### 平衡二叉树易错、易混淆点：

  - 平衡二叉树是二叉排序树的一种，因为它既保持了二叉排序树的排序属性，又保持了高度平衡性。因此平衡二叉树不是那种无所谓结点位置或排序、只要平衡性的树。
  - 给定平衡二叉树的最少结点数由一个关于h的递推表达式表达，如下左图：
    <img src=".\图片\屏幕截图 2024-09-22 083417.png" style="zoom: 33%;" /><img src=".\图片\屏幕截图 2024-09-22 083445.png" style="zoom: 33%;" />
    该公式的意思是，若只有一层深即n1，则整棵树最少有1个结点；二层深则整棵树最少2个结点；三层深4个结点...可以配合上右图理解记忆该公式，即公式中的1是根结点，另外两项分别是左子树和右子树。

### 7、b树与b+树

- b树如果按照类比的想法来理解的话，可以把它认作二叉平衡树的多叉版，这是为了使得树高进一步缩减。如图：<img src=".\图片\屏幕截图 2024-09-21 102326.png" style="zoom: 33%;" />
  其中叶子结点是失败结点，并不是实际存在的，在代码上就是NULL表示。
  
- b树有一些基本概念，关键字数指的是每个结点中存的元素数量，结点子树数量或结点分支数量指的是该结点分出去了几支，每个结点子树数量比该结点关键字数多1，这是由于每个关键词都要保证其左右都可以走的原因。m阶b树指的是该树存在的最大的分支数m。

- b树有一些核心的特性，如图所示：
  <img src=".\图片\屏幕截图 2024-09-21 103144.png" style="zoom: 33%;" />
  其中特性 1）中，之所以有m/2的规则，是为了规定b树不太窄瘦也不太宽胖，使得b树查找的效率达到一个优解。特性 2）是从平衡性的角度出发的，相比允许存在1个高度差的平衡树，为了省去一些麻烦，干脆直接定 义成绝对平衡。
  
- b树的最值问题有最小高度问题和最大高度问题，由于两个问题的求解存在相似性，因此这里只讲最大高度问题。如图：
  <img src=".\图片\屏幕截图 2024-09-21 124841.png" style="zoom: 33%;" />
  通过建立等式就可以推出图中h的表示式。图上表达式的求解思路就是归纳计算每层的最少结点数和最少关键词数，这里面隐含了一个递推关系，即每层的结点数或关键词数与上层有关。
  
- b树的插入操作遵循插满向上分裂的规则，如图演示：
  
  <img src=".\图片\屏幕截图 2024-09-21 125623.png">插入元素先像二叉排序树一样向下找位置，并且插入后一定变成一个终端结点，若遇到插满的情况，找到mid结点并提到双亲结点中找到有序的位置插入，依此循环往复。
  
- b树的删除操作遵循找先驱或找后继替代的规则，如图所述：
  <img src=".\图片\屏幕截图 2024-09-21 130008.png"  />
  值得一提的是，因为我们规定好了每个b树结点不能有低于阈值的关键词数，因此如果删除一个结点后造成低于阈值的话，需要利用借兄弟的方法来重新调整b树。若兄弟够借，那么就借兄弟的；若不够借，那么干脆合并。在这个过程中，因为要保持有序性，因此一次调整可能涉及多个结点的调整，但是总的方针是，调整后的b树，只要符合定义就可以。
  
- b+树是对b树的进一步升级，引入分块查找的概念将存储元素索引化，使得查找效率进一步提高。

- b+树和b树最大的不同是，由于索引化了，所以每个结点的关键词数和其分支数是一致的，这样才能使查找永远能正确达到终点。其次的区别是，非终端结点存储的都是索引/关键词，只有终端结点才存的是元数据。其他的不同基本都是从这两点不同引申出去的。

- b+树还有一个特性，就是因为元数据所在终端结点都在同一层，理所当然可以用链表的方式将其串联起来，因此b+树还支持顺序查找。

- #### b树与b+树易错、易混淆点：

  - 结点和关键字的分野：结点是包含关键字在内的，因此b树的结点数一定小于或等于关键字数。
  - 插入操作中的插满向上分裂规则，更细化的步骤为，先分裂成三部分，左半部分，mid，右半部分，再把mid提上双亲结点中去，最后左半部分和右半部分在同一层上变为两个结点。
  - 删除结点而变动其他结点时，要时刻想一想调整后的树是否满足特性，这里最容易忽视的特性是关键字数需要满足相应的范围。
  - 删除操作一定会导致叶结点发生变化，如果任何变化都算的话，那么是的，因为删除时会涉及合并、交换、外借等操作，最大的变化会导致树高变化，最小的变化是关键字互换。
  - 只有b+树的叶子结点用链表串起来了，而不是b树，这里记b+树原数据都在同一层就合理了。

### 8、散列表(哈希表)

- 散列表的两大要素：散列函数(哈希函数)和冲突处理方法。散列表的精髓其实就是以空间换时间，假设空间无限，那么冲突为零。

- 同义词的概念：不同的关键字通过散列函数映射到同一个值，则称这些的关键字为同义词。

- 装填因子的概念：表中每个关键字下记录数累计的总和/散列表长度。装填因子越大，发生冲突的概率越大，查找越慢。因为装填因子的大小主要由冲突数决定。

- 常见的散列函数主要有四种：除留余数法、直接定址法、数字分析法、平法取中法。
  除留余数法：设散列表长度为m，那么除余因子可以定为不大于m但接近或等于m的质数，质数是指无法被1或自身整除的数。这样设定的意义在于，质数可以平衡相较于其他除余因子的数值分布，使计算出来的哈希值是均匀的，有利于降低装填因子量级。
  直接定址法：计算出的哈希值直接就是数据存储地址，适合关键字的分布基本连续的情况，如班级的学生学号信息。
  数字分析法：基于直接定址法——针对自己数据中数字规律均匀变化的部分，让哈希值直接等于该部分数据地址。
  
- 我们对哈希值冲突避无可避，因此寻求两大种处理冲突的方法：拉链法和开放地址法。
  拉链法：拉链法的核心思想是，当发生冲突时，将冲突关键字用链表的存储结构挂起来，如图：<img src=".\图片\屏幕截图 2024-09-22 195947.png" style="zoom: 33%;" />
  开放地址法分为线性 探测法、平方探测法以及伪随机序列法。
  线性探测法：遇到冲突了就往后挪，有空地儿就占了。值得一提的是，在线性探测法下，其查找操作在遇到空位时仍没有查到，这种情况就是查找失败。那么在这种逻辑下，删除关键字就不能真正删除，应该用标志位替代关键字。线性探测法查找效率其实并不高，因为它很容易造成同义词和非同义词扎堆在一起，查找的时候需要每个辨认，那么最坏的情况下可能需要找遍所有关键字才能找到。
  线性探测法：基于线性探测法容易扎堆的缺点，平方探测法被提出。它的思想就是通过平方进一步空开关键字，如图：
  <img src=".\图片\屏幕截图 2024-09-22 202520.png" style="zoom:33%;" />
  需要注意的是，这里的平方项正负交错，这是为了使一个冲突关键字尽量往左右两边更近的地方靠，从而实现更快的查找。值得一提的是，当使用平方探测法时，散列表长度m必须是一个质数，才能探测到所有的位置，不然会重复探测某些位置。
  伪随机序列法：冲突关键字需要挪位的位数是随机数。
  
- #### 散列表易错、易混淆点：
  
  - ASL的查找失败和查找成功有所区别：查找成功的分母项是插入关键字（元素）的数目，而查找失败的分母项是除余因子（以除留余数法为例）。因为前者意味着所有关键字的查找成功的情况会被考虑，而后者是说对于不存在的关键字可以是无数个的，但这无数个一定是被映射到0-除余因子的存储区间，因此注意查找失败时，一定只管区间范围内的查找失败次数，无视范围外的探测关键字。
  - 散列表的平均查找长度与装填因子直接相关，而非与表项个数和表长直接相关。



## 第八章：排序

### 1、排序的基本概念

- 排序算法的稳定性：若一个排序算法需要处理重复元素的数据，当该算法能够保证重复元素的原本相对位置时，我们称该算法是稳定的，否则不具稳定性。
- 排序算法的主要类别分为内部排序和外部排序。外部排序指数据无法一次性放入内存的、需要特定与外存（硬盘）交换数据的排序算法。两者的关注点有一定的区别，内部排序关注时间、空间复杂度的降低，而外部排序还会重点关注降低读写磁盘硬盘的次数。
- 排序算法的时间复杂度主要考虑最好情况（原本就有序）和最坏情况（原本就逆序），平均时间复杂度就是(最好+最坏)/2。
- 一个各种算法可视化的网站：https://www.cs.usfca.edu/~galles/visualization/Algorithms.html
- 以下介绍的各种排序算法的执行步骤都基于升序排序的要求。

### 2、内部排序之插入排序

- 直接插入排序

  - 直接插入排序的思想简单来说，就是将元素列表看成有序部分和无序部分的两部分，设置一个滑动指针，需要滑动n-1趟。在每趟中，取无序部分元素和有序部分最近一个元素进行探测式的对比，如果无序部分元素比有序部分元素大，那么根据升序，就无需继续将无序部分元素与下一个有序部分元素进行对比了，直接进入下一趟；否则有序部分元素右移一位，并找下一个有序部分元素对比、位移，直到有合适的位置插入，如图。算法初始时默认第一个元素归为有序部分。
    <img src=".\图片\屏幕截图 2024-10-01 102326.png" style="zoom:33%;" />
  - 空间复杂度为o（1），因为基本都是原地执行。时间复杂度在最好的情况下，即元素本来有序时，为o（n），因为每趟比较只需要比较1次；最坏的情况下，即元素本来逆序时，为o（n平方），因为每趟都需要和有序部分比完，并且比较次数和移动元素次数基本相同。平均时间复杂度为o（n平方）。另外，该算法是稳定的，因为在比较的时候可以控制是否移动相同元素。
  - 经过分析可以发现，当有序部分足够长时，会需要比较很多次，于是引入折半查找的方法优化该比较步骤，称这种排序算法为折半插入排序。算法大致步骤与折半查找相同，然而为了保证算法的稳定性，当比较到相同元素时，不应结束查找，应继续查找比较。

- 希尔排序

  - `分析到直接插入排序对基本有序的元素列表排序效率高`，于是针对这一点提出希尔排序：其思想简单来说，就是将元素列表分出子表，相当于抽出子表的元素进行简单比较和移动，那么直到最后一轮前，元素列表就可以被调整到基本有序的状态。其效率提高的部分在于，子表的规模小，且有序性逐趟增加，节省了相较于直接插入排序所需要得大规模比较和移动。具体定义如图所述：
    <img src=".\图片\屏幕截图 2024-09-23 130240.png" style="zoom: 25%;" />

    值得注意的是，增量d既是子表的个数，又是每个子表取元素的依据，具体令i为0，d为2，那么子表1拿到的元素就是i+d->49，i+2d->27，依此类推。i的取值就是列表的索引号，任何的i+d超出列表长度时，就可以终止i的迭代。另外d是还是可以调整的参数，不同的d会很大程度的影响算法的效率。`当d=1时，其实就是对整个元素列表进行直接插入排序。`

  - 希尔排序的空间复杂度为o（1），时间复杂度根据d的选取有关，因此是不定式。但是时间复杂度的最坏情况是d=1时，等价于直接插入排序，因此这时的时间复杂度为o（n平方）。最后，该算法是不稳定的，可以用简单的如3个元素的元素表来验证。

- #### 直接插入排序易错、易混淆点：

  - 本来就有序是该算法时间效率最好的情况，因为每趟比较只需要比较1次，1次是指和有序部分最近的一个元素比较，发现可以直接下一趟，所以每趟比较1次。注意不要错以为每趟需要和有序部分所有元素对比，没必要，因为有序部分最近的元素肯定是最大的。
  - 折半插入排序应用于对有序部分元素的比较，并且由于折半的次数只与有序部分的长度有关，因此在同等长度元素表下，折半的次数是固定的，从而比较的次数也是固定的。

- #### 希尔排序易错、易混淆点：

  - 不要认为从表头开始，按增量找到相应的元素就完事了，这时候往往你才找到了第一个子表而已，需要紧接着从第二个、第三个元素...按增量找第二个、第三个...子表的元素。
  - `子表中的插入排序`在比较元素步骤时也要注意，`根本不总是无序部分元素和有序部分的所有元素会对比完`，当比较时大小达到规则就可以交换元素，交换完就不再继续这一趟的对比了，会进入下一趟的对比，因此实际上的比较次数会比完全比较次数少很多。

### 3、内部排序之交换排序

- 冒泡排序
  - 冒泡排序也分为有序部分和无序部分，有序部分在元素表头，初始时整个元素表都认作无序部分。冒泡排序的思想就是元素从后往前的两两比较，需要的时候就交换位置，使小的在前大的在后，然后按照为1的步长向前滑动，再次比较...循环往复，一直滑动到前方已经的有序部分才停下，算跑完了一趟。不难发现每一趟都将一个局部小数推到了前面，当所有趟都跑完时，局部小数从前往后依此排好，即升序排序成功。
  - 冒泡排序的结束条件是，当某趟遍历完后却没有一次交换的，可以认为整个元素表已经有序，故算法结束。
  - 冒泡排序的空间复杂度为o（1），`最好情况的时间复杂度为o（n）`，因为这时元素有序，`只需要比较一趟即可结束`；最坏情况的时间复杂度为o（n平方），需要跑n每趟，每趟交换n次。总的来说，平均时间复杂度为o（n平方）。由于该算法在比较时可以控制相同元素的比较规则，因此该算法是稳定的。
  - 值得一提的是，冒泡排序的交换操作是用temp变量实现的，因此实际上每次交换需要移动元素3次。
  
- 快速排序
  - 快速排序算法的总体思想是定下一个基准元素，依据基准元素将元素划成左右两部分，即 左<基<右（升序） 或 左>基>右（降序）。基于一次基准元素的划分完成，我们称其为“一次划分”。
  - `快速排序一般的划分策略是每次取当前表头元素作为基准元素`，然后设置一个low指针和一个high指针，初始时分别指向头尾。low和high指针按规则交替向表中方向迭代，迭代过程中需要将指针指向的元素与基准元素比较，需要交换位置的元素就直接交换到对方当前指针所指位置。当low指针和high指针重叠，证明此时已经将除基准元素外的所有元素已经完成了一次划分，自然的指针重叠位置就是基准元素，可以认为基准元素此时的位置就是排好的，无需再动。后续再将左右两部分看成新的元素表进行划分，每次划分可以新增确定一个基准元素的位置，划分的过程持续到最小单位，即新的元素表只有一个元素而无可划分时。
  - 根据以上算法步骤的描述，可以看出该算法适合使用递归的思路来实现。其中递归的部分是划分的操作。其实进一步分析可以发现，每次的划分操作相当于构造一棵二叉子树，将划分的操作持续进行下去，那么二叉子树也将向下生长，形成总的一棵二叉树。
  - 快速排序的空间复杂度为o（1），最好情况的时间复杂度为o（n*log(n)），其中n是划分操作伴生的二叉树高度，此时的每一次划分都能够将元素表分成均匀的两部分。因为划分的总次数其实就是二叉树的高度（log(n)），而每次划分中需要遍历当前子表（不超过n）。最坏情况的时间复杂度为o（n平方），如`原本就有序时`，此时的每一次划分都十分不均匀，因此二叉树的高度加高（如所有结点呈斜线排布）。空间复杂度跟二叉树高度有关，因此最好是o（log(n)），最坏是o（n）。
  - 由于每次划分是否均匀对算法效率很重要，因此可以改进选择基准元素的方法，比如选择元素均值附近的元素，或者随机选一个。
  - 最后，该算法是不稳定的，可以用简单的如3个元素的元素表来验证。不过由于该算法优秀的性能，应用非常广泛。
  
- #### 冒泡排序易错、易混淆点：

  - 最好情况的时间复杂度为o（n），因为这时元素有序，只需要比较一趟即可结束，又因为当某趟跑下来发现没有发生一次交换，完全可以认为此时元素表已经有序，无需开启下一趟。这个判定在任何时机都是成立的。

- #### 快速排序易错、易混淆点：
  
  - 模拟快速排序算法时，注意要将基准元素抽出来让出一个空位，交换时是将元素换到空位上，每次划分最后一个填补空位的元素一定是基准元素。如下图：
    <img src=".\图片\屏幕截图 2024-09-26 094233.png" style="zoom:33%;" />
  - 一个极易混淆的点：low、high指针迭代的依据不是严格的你来我往，它真正的依据是是否指到了空位，若指到了，则该指针按兵不动，让对面指针移动，直到对面指针指向了因交换而产生的空位，则移动权滚回该指针。
  - 另一个极易混淆的点：low、high指针比较元素的规则对称相反——对于升序排序，low指针与基准元素比较时，low指针比基准元素还low才是正常的，否则需要交换；high指针与基准元素比较时，high指针比基准元素还high才是正常的，否则需要交换。
  - 每次划分，当中每个非基准元素与基准元素需要比较一次。另外我们说最好情况是指均匀划分的情形，而不是只需一次划分就有序的情形，因为快速排序算法是递归的，只要元素表长大于2，无论如何都需要多次划分。

### 4、内部排序之选择排序

- 简单选择排序
  - 简单选择排序和冒泡法类似，核心思想都是每趟将当前最小（升序）元素冒到表头，但该算法不同的是“冒泡”的策略，冒泡排序策略是每两两比较，线性的冒到表头，但简单选择排序策略则舍弃了两两比较的做法，直接选择当前最小的元素，直接提到表头。
  - 简单选择排序算法空间复杂度为o（1），时间复杂度没有最好最坏情况，算法一定需要n-1趟处理，每趟对比无序元素个数那么多次，因此n-1趟下来运算总次数形成一个等差数列求和，故时间复杂度为o（n平方）。
  - 最后，该算法是不稳定的，可以用简单的如3个元素的元素表来验证。
  
- 堆排序
  - 堆排序的起源是简单选择排序。选择排序中我们需要指定选择比较的策略，自然的，联想到一种特定的二叉树可以很快的选择到关键字，即对于 左右儿子结点<根 或 左右儿子结点>根 的二叉树，我们可以直取根结点作为被选择的关键字。然而，需要排序的关键字是顺序表的存储结构，而二叉树作为一种逻辑结构，怎么转化以符合顺序表的操作呢？自然的想到，完全二叉树可以顺序存储——按树的层次存储即可。并且由于完全二叉树的特性，我们可以随机访问树中各个结点。
  - 在堆排序算法中，我们将任意一个初始元素列表看作为一棵未经改造的完全二叉树的顺序存储表，并称之为堆。我们的目标是将一个初始堆改造称符合大小规定的堆，若堆符合 左右儿子结点<根，我们称为大根堆；若堆符合 左右儿子结点>根，我们称为小根堆。
  - 以升序排序为例，堆排序主要分为三个步骤——第一是将初始元素列表改造成大根堆（建堆）；第二是选择大根堆的第一个元素即最大元素与末尾元素交换；第三是排除已排好元素，微调那交换后可能产生破坏的大根堆，以重新符合大根堆的定义。
  - 对于第一步建堆，目标首先是对初始完全二叉树的非终端结点动土，因为非终端结点是每棵子树的根，我们需要让根节点是最大关键字；而叶子结点没有孩子，因此无所谓根不根的，不需要调节。其次就是遍历每个非终端结点，先比较两个孩子大小，取其大者，再将大者与该根节点比较，使更大者保持在根节点，算下来每棵子树需要比较两次。当发生交换而破坏了下层子树的大根堆特性时，立刻进一步对该下层子树调节，由于在大根堆中只存在根节点向下交换，因此我们将这样的调节称为“下坠”操作。
  - 堆排序算法的空间复杂度为o（1），时间复杂度是o（n*log(n)），分析如下：
    <img src=".\图片\屏幕截图 2024-09-26 110610.png" style="zoom:33%;" />
    建堆（即将初始元素列表改造成大根堆）的时间复杂度为o（n），建堆操作只运行一次；建堆完成后，在其基础上开始选择排序，总共需要n-1趟（每趟确定一个元素位置），此外，每趟中还可能涉及调整大根堆，这种调整或下坠最多会下坠完全二叉树的高度那么多层，即h层，因此每一趟的时间复杂度为o（h）= o（log（n）），于是n-1趟的时间复杂度为o（n * log（n））。最后整合所有的运行操作，时间复杂度应是o（n）+ o（n * log（n））= o（n * log（n））。
  - 堆排序还支持对堆的插入操作，具体就是将插入元素放表尾，然后应用和双亲节点比较的调整方法（上升或下坠）使堆重新规范。然而，与插入不同，删除操作用的是排序中微调的调整方法，另外删除一个元素后，会用表尾元素顶替该删除元素，然后调整。
  - 最后，该算法是不稳定的，可以用简单的如3个元素的元素表来验证。

- #### 堆排序易错、易混淆点：

  - 手算初始建堆时，第一步将元素表二叉树化，但是别把问题复杂化了，直接元素表从左到右而二叉树逐层堆满即可。
  - 建堆时，应从序号最大的非终端结点处开始，这样一定程度上能够保证后面的调整不会扰乱前面的调整。
  - 计算堆操作过程中的比较次数、交换次数时，一定还要算失败的那一次，这样才完整。
  - 堆的插入和删除操作的调整方法是不一样的，尤其是插入，调整时它只需与双亲结点比较；删除操作也区分，它只需关注调整被改动结点带来的影响，而无需像建堆时那样每个非终端结点都检查一遍。

### 5、内部排序之归并排序

- 归并排序也是一种动态规划算法，其子问题被定义成为对两个元素表进行选择合并排序，其递推条件就是元素表二分。

- 归并排序主要是两大要素：第一是对任意两个元素表进行选择合并排序的函数，第二是用递归的方法不断二分元素表，直到不能再分，然后在最小二分层次的这对子表应用选择合并排序函数，可以看作这对子表合并成一个，紧接着可以将该合并子表与相邻子表继续合并...这样逐层合并，直到所有子表合并完成。

- 合并排序的函数主要依赖于一个和传入元素表（可以是子表也可以是原生元素表）相同长度的辅助空数组，还需要定义面向元素表的low、mid、high三指针，其中mid指向二分位置。以及面向辅助数组的两个移动指针i、j，分别指向二分子表的首元素。首先将元素表复制到辅助空数组，在辅助空数组上移动，而元素表就用在辅助数组比较排序好的元素依次覆盖填充。具体来说，i和j大致交替移动，每次需要对比i和j所指的元素比大小，指针更小的就选择到元素表中填充，并且该指针后移。若某个子表已经超出其长度，则无需比较有序排放。

- 二路归并排序算法的空间复杂度为o（n），因为需要开辟一个长度为元素数量n那么长的辅助数组；时间复杂度是o（n * log(n)），因为对元素表进行递归的二分，本质上就像是构建一棵倒立的二叉树，因此其高度log(n)-1就是合并排序函数执行的趟数，而合并排序函数的时间复杂度为o（n），因此合起来时间复杂度为o（n * log(n)）。

- 最后，由于该算法在比较时可以控制相同元素的比较规则，因此该算法是稳定的。

- #### 归并排序易错、易混淆点：

  - 要认清归并排序算法的核心思维图像：对有限个有序子表进行归并排序，注意是有序子表，之所以这点容易混淆，是因为代码层面用二分的方法，分到最小元素以进行归并时，即第一趟归并，感觉不像是有序的。然而第一趟可以视作子表只有唯一元素，因此有序。
  - 二路归并排序算法的趟数是log(n)不假，但是推理的时候有些细枝末节，容易造成推导错误，重新或强调分析如下：
    <img src=".\图片\屏幕截图 2024-09-29 092357.png" style="zoom:33%;" />
    如图可知，二路归并可以看作倒二叉树，但是归并趟数明显是树高 h-1，而不是想当然的直接算作树高。进一步的，可以发现元素表长度 n 体现在倒二叉树的叶子结点层，即第一层，因此可以用叶子节点个数 n 与树高 h 的数量关系得 h-1 = log(n)，那么由于归并趟数为 h-1，因此趟数等于 log(n)。
  - 要分清楚手算模拟的归并排序思维图像和算法代码的思维图像：手算模拟还是如上图所示，对于二路排序，从左到右每数两个来排序就完事，但是算法代码的步骤不同，算法是先把整个元素表二分，在递归的执行逻辑下，归并的顺序可以等价为，先对左半部分进行完全归并排序，再对右半部分进行完全归并排序，最后两部分总的进行一次归并排序。
  - 其实手算模拟和算法代码的执行步骤是等价的，但是由于不同的执行特点，产生了表面上的不同。比如，在手算模拟中是直接对整个表进行一趟一趟的处理，而算法代码是分成两部分，每部分进行一趟一趟的处理，最后再合起来。
  - mid指针所指元素要算进前部分子表中。
  - i、j指针的移动不是严格交替的，而是哪个指针的元素被选取填充了，那就哪个指针移动。
  - 归并排序算法比较次数最少的情形是【1，2，3，4】和【5，6，7，8】合并，次数最多的情形是奇偶子表即【1，3，5，7】和【2，4，6，8】合并。前者因此子表1指针移动完全，而子表2指针不动，又因为指针移动次数和元素比较次数量级相同，因此元素比较次数为n，n为子表长度；而后者i、j指针交替移动，因此两个子表都会移动完全，因此比较次数在2n的量级，具体现场推算。

### 6、内部排序之基数排序

- 基数排序的思想另辟蹊径，它将每个元素的比较语义进一步划分，变成最小的比较语义，并利用新的语义规律来进行排序。往抽象了说，就是换个角度看问题，虽然规律发生变化，但新的规律有利于简化问题，同时能够精准刻画问题。

- 基数排序主要适用于链式存储。其算法步骤如下：首先初始化，设置r个空队列，r指代基的最大取值范围，而基（或者说关键字拆分之后的一份）又代表了最小的比较语义；
  其次是确定基的权重，权重小的基先计算，权重大的基先计算，因为越到后面计算的基，就越不容易被前面计算的基改变，因此最后计算的基一定是严格有序的。以基为单位的计算我们称为一趟，因为关键字可以拆分成d个基，因此一共会有d趟。
  最后是在每一趟中分配、收集，分配指对应的基按照其取值分配到对应的队列，收集指的是对分配好的队列重新有序收集元素，变成新的序列，供下一趟进行排序。以百位数为例，d为3，r为10，如图：
  <img src=".\图片\屏幕截图 2024-09-27 125536.png" style="zoom: 33%;" />
  
- 基数排序算法的空间复杂度为o（r），因为需要r个辅助队列，时间复杂度为o（d*（n+r）），其中n是元素个数。因为基数排序算法一趟分配o（n），一趟收集o（r），而总共有d趟分配、收集，因此形成了以上时间复杂度。

- 基数排序虽然看起来拥有优秀的排序性能，但根据时间复杂度可知，若d和n的规模很大，那么其时间复杂度会退化为o（n平方），所以我们应该根据实际问题的规模来决定是否使用基数排序。如图：
  <img src=".\图片\屏幕截图 2024-09-27 130059.png" style="zoom:33%;" />
  
- 最后，由于该算法在分配、收集步骤是按顺序入队的，因此可以保持元素的相对位置，故该算法是稳定的。

- #### 基数排序易错、易混淆点：

  - 把上述的各种数量关系再仔细认清楚、记清楚就好了，尤其是开辟队列的个数应是基的最大取值范围。
  - 基数排序不是基于比较关键字的排序算法，注意不要被带偏了。

### 7、外部排序之多路归并排序

- 外部排序的目标就是将外部磁盘的记录按关键字进行排序，主要运用的是多路归并排序算法，本小节又主要以二路归并为讲解示例。多路归并排序是外部排序之母，本笔记后续的外部排序算法都基于多路归并排序进行优化。

- 外部排序的基本思想和归并排序类似，但需要根据读写磁盘以及内存分配来调整步骤，主要分为外部读取、内部（内存区）归并排序两大步骤，具体思维图像如图：
  <img src=".\图片\屏幕截图 2024-09-29 114516.png" style="zoom:33%;" />

  左图中绿色部分是磁盘的最小单位存储块，在此例中块可以存储3个记录。右图中配备了两种缓冲区，每个缓冲区单位的大小都和磁盘块大小相等。其中输入缓冲区用于归并排序。初始我们需要依次从磁盘中读取和输入缓冲区对应数量的块，以在内部进行归并排序，每趟排序完成后再借由输出缓冲区写回外部相应位置，这一个过程能使每个块内部的记录变得有序、块间也有序。

- 初始化结束后，面对有序的存储块，接下来的操作就是一步步的归并排序，每次归并本质上就是合二为一，然后写回磁盘区，变成新的归并段，知道最终合并成唯一的归并段，如图：
  <img src=".\图片\屏幕截图 2024-09-29 214552.png" style="zoom:33%;" />
  当归并段越来越大时，看似输入缓冲区无法纳入处理，其实还是可以一块一块来：每次从俩取归并段中取最小一块送去排序，排好一块立即按照从上到下从左到右（升序任务下）写回，或者某输入缓冲区为空，就再从归并段中写入，直到所有块都被处理完成。

- 二路的外部归并排序时间效率分析如图：
  <img src=".\图片\屏幕截图 2024-09-29 220306.png" style="zoom:33%;" />

  其中读写外存时间开销分为生成初始归并段、归并趟数两个头目，而这两个头目又与块数量有关，具体可从上图看出，与排序趟数相关性最大。内部排序时间是指初始化将每存储块变得有序的时间开销。由时间开销表达式可知，最影响效率的是读写外存，因此我们应该想办法减少排序趟数，因此最直接的办法就是多路归并，即增加输入缓冲区的数量，从图形上看，就是归并排序的倒k叉树加宽变矮了，这样不仅可以使得每个初始归并段变得更大，从而助力减少归并趟数，而且除了初始归并段，后续归并容量也变大，进一步减少归并趟数。如图：
  <img src=".\图片\屏幕截图 2024-09-29 222216.png" style="zoom:33%;" />

- 然而多路归并的缺点是会占用更大内存，最要命的是多路归并时需要对比关键字的次数大大增加，因为需要从k路中逐一对比关键字。

### 8、外部排序之败者树

- 本笔记的败者树算法是为了解决多路归并时对比关键字次数增加的问题的。
- 败者树的思想是利用二叉树的特性，减少没必要的比较，放弃“全连接式”的比较方法，将未知元素只将最值元素比较就可得出该元素的大小。具体来说来，败者树可以比较明显的分为两大步骤：第一初始构造一棵败者树，第二根据败者树从归并段中逐个取元素投入败者树，获得快速的比较结果。整个过程可以想象是，一个修路车推进修路，修好了他是第一个到达的，而有了这条路，后续更多车可以高速抵达。
- 初始化构造败者树如图：
  <img src=".\图片\屏幕截图 2024-09-30 122935.png" style="zoom:33%;" />
  k个归并段在这里指k路或内存k个输入缓冲区。初始构建败者树时从每个归并段取1个元素，两两比较，胜出的结点在树中上升，继续和邻近胜出结点比较，每次比较在双亲结点中记录败者结点所在归并段序号。循环往复，直到推举最值元素来到根结点位置。最后我们取出根结点记录，根据记录取出相应归并段的当前元素，并放入输出缓冲区。值得一提的是，上图中绿色的叶子结点不算作败者树结点，败者树只记录败者所在归并段的序号。
- 初始化败者树后，取上次被取走元素所在归并段的下一个元素补位，开启第二轮轮比较，记录败者，将最值元素推向根结点，然后取出...
- 分析败者树的时间效率，在第一步初始构造一棵败者树中，需要对比关键字k-1次，而第二步在有了败者树后，将元素推向根节点只要爬灰色结点那几层，因此只需要对比关键字 log(k) 次（k是k路，也是上图绿色结点个数，具体推算与归并排序时间效率分析相似）。

### 9、外部排序之置换选择排序

- 置换选择排序的出发点是进一步减少初始归并段的数量，虽然多路归并排序已经可以减少很多了，然而这仍然很受限于内存大小，不灵活且初始归并段的数量仍较多。

- 置换选择排序的思路是，分出三个区间——工作区、排序区、待排序区，排序区又可以分为若干个归并段子区间。我们可以最少利用一个输入缓冲区作为工作区，初始工作区为空、排序区为空，待排序区挤满。先从待排序区就近拿出一些元素载入到工作区，开始进行选择：先选择最小的元素到排序区（升序），并用MINIMAX变量记录该元素。然后从待排序区就近取一个元素补位，尽量保证每次选择时工作区被占满，再一次选择最小的元素，更新MINIMAX...当某个从待排序区取出的元素作为最小元素却比MINIMAX还要小时，我们可以认为该元素继续加入当前归并段会破坏该归并段的升序有序性，因此我们可以暂时封印住该元素，如图红色元素：
  <img src=".\图片\屏幕截图 2024-09-30 213109.png" style="zoom:33%;" />

  封印后继续进行选择排序，直到内存工作区被红色类元素占满，我们可以认为是时候另开一个归并段了，这时将红色元素解禁，MINIMAX置零，而后继续选择排序，不过选择的元素需要归到第二归并段...循环往复。

- 如果要达到置换选择排序的优解，我们认为至少要设置两块内存缓冲区，其中一块作为选择排序的工作区，另外一块用于存放从磁盘读入的数据元素。因为假设我们只有一块内存缓冲区，即仅设置工作区，那么每个元素就需要专门去磁盘中读取，n个元素就读取n次。再假设回来，有了另一块输入缓冲区，那么每次去磁盘中可以直接读取一块元素，而不是一个元素，假设每块3个元素，那么读磁盘的次数能减少2倍，当有更多内存缓冲区时，节省的次数更可观。
  写入磁盘同理，若提供更多输出缓冲区，那么写入次数也将大大减少。

### 10、外部排序之最佳归并树

- 最佳归并树从对置换选择排序算法进一步改进得来的。根据置换选择排序的特点，每个归并段元素数量是不定的，依据初始归并段进行后续归并排序，估计该算法的读写磁盘次数是困难的，因为不同的归并顺序会带来差异性的时间效率，于是我们需要找到最优的归并顺序。
- 若我们对置换选择排序算法得来的归并段进行归并排序，通过时间效率分析可得一个重要结论，如图：
  <img src=".\图片\屏幕截图 2024-09-30 225209.png" style="zoom:33%;" />
  其中R结点是初始归并段的抽象表示，其数值表示R的长度。从图中可以得出，读、写的次数是合并的归并段总长度，因为这里认为是一个一个从磁盘读写的，而不是一块一块、几块几块的。根据这个结论，我们可以进一步确定优化目标：减少归并树的WPL。
- 而减少WPL当然不能忽略哈夫曼树方法，结合哈夫曼树就是最佳归并树的精髓。因此对于二路最佳归并树，复用哈夫曼树知识点即可。
- 当然最佳归并树的方法可以推广到多路（k路）归并，具体与二路最佳归并树的构造十分类似，只是从每次选R最小的2个结点变成了每次选R最小的k个结点进行合成而已。然而，k路最佳归并树有一种特殊情况需要处理，如图：
  <img src=".\图片\屏幕截图 2024-10-01 084604.png" style="zoom:33%;" />
  其中严格的k叉归并树是指该归并树只有度为0和度为k的结点，换言之除了叶子结点外，其他所有结点都具有k个分支。而之所以我们要求构造严格的k叉归并树，是因为其WPL总是最小。很明显我们可以得出，是否构成严格的k叉归并树由归并子段的数量决定。然而，归并子段的数量没有倍数规律，因此有时候无法构造严格k叉归并树，那么自然我们想出利用空结点补足归并子段数量以满足倍数规律，即补“虚段”。
- 补“虚段”的个数满足以下数量关系：
  <img src=".\图片\屏幕截图 2024-10-01 090628.png" style="zoom:33%;" />
  其中 n0 是严格归并树的初始归并段数量，因为对于最佳归并树（某种意义上的哈夫曼树）来说，只有叶子结点才是真正的归并段，如该小节的第一张图。k * Nk = n - 1 是k叉树的分支边总数量的关系恒等式。除得尽是指Nk作为结点数量一定是个整数。
  图中①作为判定定理，其中初始归并段数量并不是严格归并树的，而是给定的数量。该条定理可以理解为，不需要添加虚段时，证明可以直接构成严格归并树，因此Nk是除得尽的。
  图中②作为判定定理，可以换个角度理解：若余数不为0，证明无法直接构成严格归并树，因此需要添加虚段，当我们在分子部分加上的虚段数量，再除余得0时，说明添加的虚段数量是正确的。



## 附录

### ASCII表
<img src=".\图片\ascii-table-alpharithms-scaled.jpg">



### 树与二叉树的数量关系总览

- `结点和边`：一棵树有n个结点的话，那么会有n-1条边

- `度和第i层结点数`：只能求至多情况下的关系，至少的情况第i之前的层没有规律。可以想到若是第i之前的层都是度达到最大值m的结点，这就是至少的情况。因此，每层的结点数量排列起来是一个公比为m的等比数列。

- `高度和n叉树的结点数`：先讨论最多结点数——n叉树在h高度里每层结点的度都是满的，因此每层的结点数量排列起来是一个公比为m的等比数列，利用等比数列求和公式可以求出前h层的结点总数。

- `度、结点数和最小高度`：基本思想是树长得越宽，在固定结点树下高度就会最小。由于树在这里要求为宽体树，则先考虑的高度h-1的结点数为前h-1层的等比求和。然而还有第h层不能忽视了，第h层最多（不超过）有层满那么多结点，因此可得一个不等式可出解高度。

- `度、结点数和最大高度`：基本思想是树长得越窄小，在固定结点树下高度就会最大。由于树在这里要求为窄体树，因此每层的结点数越少越好，那么就设每层结点数为1就行了。然而为了满足树的度，因此有一层的结点数与数的度相同。最后总结起来可以推得一个度、结点数和最大高度的等式关系式。`取整的方向是否需要注意？`

- `二叉树叶子结点数和度为2的结点数`：可以用下式推导出来。
  $$
  \begin{cases} 
    n_0=n_2+1 &&&\\
    0 \cdot n_0+1\cdot n_1+2 \cdot n_2=n \\
    n_0=n-n_1-n_2
  \end{cases}
  $$

- `结点数和完全二叉树的高度`：这里没有至多至少一说，因为完全二叉树前n-1层是本来就是满的。因此求高度有两种角度，第一种角度是结点总数等比求和的方法。第二种方法是最后一层和倒数第二层结点的数量关系方法。